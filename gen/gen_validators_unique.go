package gen

import (
	"fmt"
	"strings"

	"github.com/ogen-go/ogen/gen/ir"
)

// generateUniqueValidators generates validateUnique[TypeName]() functions
// for arrays of complex types that require hash-based duplicate detection.
func (g *Generator) generateUniqueValidators(fs FileSystem, pkgName string) error {
	if len(g.equalitySpecs) == 0 {
		return nil
	}

	var b strings.Builder

	// File header
	fmt.Fprintf(&b, "// Code generated by ogen, DO NOT EDIT.\n\n")
	fmt.Fprintf(&b, "package %s\n\n", pkgName)
	fmt.Fprintf(&b, "import (\n")
	fmt.Fprintf(&b, "\t\"github.com/ogen-go/ogen/validate\"\n")
	fmt.Fprintf(&b, ")\n\n")

	// Generate function for each type
	for _, spec := range g.equalitySpecs {
		writeValidateUnique(&b, spec)
	}

	filename := fmt.Sprintf("%s_validators_unique_gen.go", snakeCase(pkgName))
	return writeFormattedCode(fs, filename, []byte(b.String()))
}

// writeValidateUnique generates a single validateUnique[TypeName]() function
func writeValidateUnique(b *strings.Builder, spec *ir.EqualityMethodSpec) {
	typeName := spec.TypeName

	fmt.Fprintf(b, "// validateUnique%s checks for duplicate items using hash-based detection.\n", typeName)
	fmt.Fprintf(b, "func validateUnique%s(items []%s) (err error) {\n", typeName, typeName)
	fmt.Fprintf(b, "\tif len(items) <= 1 {\n")
	fmt.Fprintf(b, "\t\treturn nil\n")
	fmt.Fprintf(b, "\t}\n\n")

	// Depth limit panic recovery
	fmt.Fprintf(b, "\tdefer func() {\n")
	fmt.Fprintf(b, "\t\tif r := recover(); r != nil {\n")
	fmt.Fprintf(b, "\t\t\tif e, ok := r.(*validate.DepthLimitError); ok {\n")
	fmt.Fprintf(b, "\t\t\t\terr = e\n")
	fmt.Fprintf(b, "\t\t\t} else {\n")
	fmt.Fprintf(b, "\t\t\t\tpanic(r)\n")
	fmt.Fprintf(b, "\t\t\t}\n")
	fmt.Fprintf(b, "\t\t}\n")
	fmt.Fprintf(b, "\t}()\n\n")

	// Hash bucket structure
	fmt.Fprintf(b, "\ttype entry struct {\n")
	fmt.Fprintf(b, "\t\titem  %s\n", typeName)
	fmt.Fprintf(b, "\t\tindex int\n")
	fmt.Fprintf(b, "\t}\n")
	fmt.Fprintf(b, "\tbuckets := make(map[uint64][]entry, len(items))\n\n")

	// Duplicate detection loop
	fmt.Fprintf(b, "\tfor i, item := range items {\n")
	fmt.Fprintf(b, "\t\thash := item.Hash()\n")
	fmt.Fprintf(b, "\t\tbucket := buckets[hash]\n\n")
	fmt.Fprintf(b, "\t\tfor _, existing := range bucket {\n")

	if spec.NeedsDepthTracking {
		fmt.Fprintf(b, "\t\t\tif item.Equal(existing.item, 0) {\n")
	} else {
		fmt.Fprintf(b, "\t\t\tif item.Equal(existing.item) {\n")
	}

	fmt.Fprintf(b, "\t\t\t\treturn &validate.DuplicateItemsError{\n")
	fmt.Fprintf(b, "\t\t\t\t\tIndices: []int{existing.index, i},\n")
	fmt.Fprintf(b, "\t\t\t\t}\n")
	fmt.Fprintf(b, "\t\t\t}\n")
	fmt.Fprintf(b, "\t\t}\n\n")

	fmt.Fprintf(b, "\t\tbuckets[hash] = append(bucket, entry{item: item, index: i})\n")
	fmt.Fprintf(b, "\t}\n\n")
	fmt.Fprintf(b, "\treturn nil\n")
	fmt.Fprintf(b, "}\n\n")
}
